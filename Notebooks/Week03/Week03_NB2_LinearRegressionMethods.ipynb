{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04bfa1b8-7c3a-4fdc-bb21-a42ef07591e0",
      "metadata": {},
      "source": "# ML training:  Exact vs Iterative & Scikit-Learn"
    },
    {
      "cell_type": "markdown",
      "id": "5dc3c951-3a28-4187-b7f5-7c92ff1d3886",
      "metadata": {},
      "source": "Acknowledgement: This notebook was derived in part from A. Geron's materials: https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d00c71-08b2-4eb6-8d92-aa0481ba60a8",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt"
    },
    {
      "cell_type": "markdown",
      "id": "de08088e-0e3e-42d8-8ca9-b888dac52e9e",
      "metadata": {},
      "source": "### Get the data\nHere we're manufacturing it."
    },
    {
      "cell_type": "markdown",
      "id": "904a6333-8acf-48bf-9453-f05a1e36d98e",
      "metadata": {},
      "source": "We're going to keep this simple:\n\n$$ y(x) = 4 + 2x + \\text{noise}$$"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87be26f7-dce8-440e-941d-a9c898d4da36",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# this is our array of x values\nx = np.linspace(0, 10, 50)\n\n# this y is the underlying linear function\ny_underlying = 4 + 2*x\n\n# generate 50 points of noise from a normal \n# distribution that has mean = 0 and std dev = 1.5\nnp.random.seed(42)\nnoise = np.random.normal(0,1.5,50)\n\n# this y is the theoretical value + noise\ny = y_underlying + noise"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a13fe04-d510-484d-ae23-15d61cbd52f6",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# plot our theory curve\nplt.plot(x,y_underlying,'k')\n\n# plot our data generated from the theory curve + noise\nplt.scatter(x,y,color='k',marker='o')\n\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "id": "b22c675c",
      "metadata": {},
      "source": "# Using machine learning\n\nWhen we have data and we want to use it to make predictions, or when we want to learn the best parameters for modeling relationships between our dependent and independent variables, we will not have something like \"y_underlying\".  We will only have \"y\".\n\nOur \"y\" here is intended to represent a real noisy set of values of our dependent variable.  On the basis of our noisy \"y\" and the values of our independent variable \"x\", we can use machine learning to determine an optimum set of values for modeling the data.  \n\nFor linear regression, that is equivalent to finding the $\\Theta$ that optimizes how well the data is fit when we specify that \n\n$$y_{pred} = \\bf{\\theta^T x}$$ \n\nor in our case \n\n$$y_{pred} = \\theta_0x^0 + \\theta_1x^1$$ \n\nor as commonly written for linear equations, \n\n$$y_{pred} = mx+b$$  \n\nHere, we're going to compare the $m$ and $b$ that we find from using machine learning to the $m=2$ and $b=4$ from our underlying linear equation that was used to generate the data."
    },
    {
      "cell_type": "markdown",
      "id": "1ea90737-b5a5-4a33-9994-478ce6f1ca58",
      "metadata": {},
      "source": "## Exact solution"
    },
    {
      "cell_type": "markdown",
      "id": "05c26735-69bd-4eef-8457-8d88b93f35ed",
      "metadata": {},
      "source": "* To get the bias term, we make array elements whose first term is 1 (for the bias) and whose second term is x (for the x-value)\n* We also reshape y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a06ba25",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295f9728",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d038fe35-aab9-4843-815e-77cca8c09af5",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "X_b = np.c_[np.ones((50, 1)), x] \ny = y.reshape(-1,1)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f94ed24",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "X_b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0640fe",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "y"
    },
    {
      "cell_type": "markdown",
      "id": "74445032-11ac-4808-894a-cd2243336d7b",
      "metadata": {},
      "source": "**The analytical solution**\n\n$\\theta = (X^TX)^{-1}X^Ty$\n\nThis is a straight-forward and direct calculation of $\\theta$.  There isn't any optimization here, there's no trying to minimize the cost function, it's an exact solution.  However, it does rely on inverting a matrix, which can potentially be time-consuming and be ill-defined for certain types of matrices."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc3ef52-1389-4646-8687-f97e234fe3e4",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "431954fe-9366-4841-8b31-77fb4e5e46c6",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "theta_best"
    },
    {
      "cell_type": "markdown",
      "id": "65fb90f1",
      "metadata": {},
      "source": "The above does not give exactly $m=2$ and $b=4$, but it gives the best solution for $\\theta$ on the basis of the information contained in our noisy set of data.\n\nTo see how it compares with the data, we'll make a plot of the line with a scatter plot of the data."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b15746a2-561c-458a-856c-51a9b9891662",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# Make a two-element array at the min and max of the x range.\nX_new = np.array([[0], [10]])\nX_new_b = np.c_[np.ones((2, 1)), X_new]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "287fe80b-b54d-47de-9668-05fffb76f6df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "X_new_b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6bd3fb-0af0-458c-a6e7-c40b15394c23",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "y_predict = X_new_b.dot(theta_best)\ny_predict"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f5a0f08-d662-4ea3-b2d7-d4e460af2f46",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.plot(X_new, y_predict, \"r-\")\nplt.plot(x, y, \"b.\")\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "id": "34fba16a-cf59-4e96-9e29-a61cf1b717cc",
      "metadata": {},
      "source": "## Doing this with Stochastic Gradient Descent (SGD)\n\nSGD is a method that uses iteration to converge towards optimum values for $\\theta$.\n\nWe'll start by randomly getting 2 values to initialize $\\bf{\\theta}$ (for $\\theta_0$ and $\\theta_1$, or equivalently for $b$ and $m$, depending on how you write the coefficients in the linear equation)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d75849e9-f5a5-4b61-ac92-7df65a5f5ae4",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "np.random.randn(2,1)"
    },
    {
      "cell_type": "markdown",
      "id": "50a8d752",
      "metadata": {},
      "source": "### Batch Gradient Descent\n\nFor batch gradient descent, one iteration of updating the parameters is carried out by training on the entire data set.\n\nThe coefficients for our linear equation are updated using gradient descent\n\n$$ \\theta = \\theta - \\eta \\frac{\\partial J}{\\partial \\theta} $$\n\nwhere $\\eta$ is the learning rate and $J$ is the cost function that we are trying to minimize.  Here we use a cost function appropriate to the mean squared error."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bee994d",
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": "X_b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111f2c74-5ce6-4e9a-9d8e-1a0df7b970d3",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "eta = 0.01  # learning rate\nn_iterations = 1000\nm = 50\n\ntheta = np.random.randn(2,1)  # random initialization\n\nfor iteration in range(n_iterations):\n    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n    theta = theta - eta * gradients"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e3efb82-2ceb-47c3-a343-d10b9ca89dc6",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "theta"
    },
    {
      "cell_type": "markdown",
      "id": "67030041",
      "metadata": {},
      "source": "The above gives good agreement with our expected $m = 2$ and $b = 4$.\n\nLet's plot the evolving linear fit for different values of $\\eta$."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a314c7-763f-42d0-8931-642799070962",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "for i in [0.001, 0.01, 0.05]:\n    eta = i  # learning rate\n    n_iterations = 1000\n    m = 50\n\n    theta = np.random.randn(2,1)  # random initialization\n\n    for iteration in range(n_iterations):\n        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n        theta = theta - eta * gradients\n        if iteration < 20:\n            X_new = np.array([[0], [10]])\n            X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n            y_predict = X_new_b.T.dot(theta)\n            plt.plot(X_new, y_predict, \"r-\")\n            plt.plot(x, y, \"b.\")\n    plt.show()"
    },
    {
      "cell_type": "markdown",
      "id": "a5ad3f1b",
      "metadata": {},
      "source": "### Stochastic Gradient Descent\n\nFor stochastic gradient descent, one iteration of updating the parameters is carried out by training on a single point in the data set, rather than updating with *all* of the data at once.  This is a faster process but more stochastic.\n\nIn the code below, we also use a learning schedule hyperparameter.  This decreases $\\eta$ after every update, so that the coefficients are more likely to converge toward the minimum rather than randomly bouncing around the minimum or even shooting over to another minimum.  On the other hand, it means that this method may be slower to converge if the learning becomes too slow before it actually gets to the minimum."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a81e27-69f3-47a3-a6a4-9b71d98817e6",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "theta_intercept_sgd = []\ntheta_slope_sgd = []\nm = len(X_b)\nnp.random.seed(42)\n\nn_epochs = 100\nt0, t1 = 5, 50  # learning schedule hyperparameters\n\ndef learning_schedule(t):\n    return 0.2 * t0 / (t + t1)\n\ntheta = np.random.randn(2,1)  # random initialization\n\nfor epoch in range(n_epochs):\n    for i in range(m):\n        if epoch == 0 and i < 20:                    # not shown in the book\n            y_predict = X_new_b.T.dot(theta)           # not shown\n            style = \"b-\" if i > 1 else \"r--\"         # not shown\n            plt.plot(X_new, y_predict, style)        # not shown\n        random_index = np.random.randint(m)\n        xi = X_b[random_index:random_index+1]\n        yi = y[random_index:random_index+1]\n        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n        eta = learning_schedule(epoch * m + i)\n        theta = theta - eta * gradients\n        theta_intercept_sgd.append(theta[0][0])\n        theta_slope_sgd.append(theta[1][0])                 # not shown\n\nplt.plot(x, y, \"b.\")                                 # not shown\nplt.show()                                           # not shown"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2edd766-fd4c-4c46-843f-510b0d7ee3e0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "theta"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74f37bd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "len(theta_slope_sgd)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdc661d",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "fig,ax = plt.subplots(2,1)\nax[0].plot(theta_intercept_sgd)\nax[1].plot(theta_slope_sgd)"
    },
    {
      "cell_type": "markdown",
      "id": "332f6505-f2f1-4ab9-923a-6b41dc02321d",
      "metadata": {},
      "source": "## The Scikit-Learn way for SGD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c1f7d24-55a1-445d-841f-6f2f3de94202",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "from sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\nsgd_reg.fit(x.reshape(-1,1), y.ravel())"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10fc37ae-94ea-49bf-b7c2-2e00f335e143",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "sgd_reg.intercept_, sgd_reg.coef_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24b3846",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.plot(x.reshape(-1,1), \n         y.ravel(),\n         'b.')\nplt.plot(np.array([[0], [10]]), \n         sgd_reg.predict(np.array([[0], [10]])), \n         'r-')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f770ab4",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(x.reshape(-1,1), y.ravel())"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e182a2b6",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "lin_reg.intercept_, lin_reg.coef_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1de64d",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.plot(x.reshape(-1,1), \n         y.ravel(),\n         'b.')\nplt.plot(np.array([[0], [10]]), \n         lin_reg.predict(np.array([[0], [10]])), \n         'r-')\nplt.plot(np.array([[0], [10]]), \n         sgd_reg.predict(np.array([[0], [10]])), \n         'g--')"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}