{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fda0961f",
      "metadata": {},
      "source": "# Illustrating Principal Component Analysis (PCA) with Penguins"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d5df81",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# Execute this cell to import libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91cef64a",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "x1 = np.linspace(1,100,100)\nx2 = x1 + np.random.normal(0,2,100)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b31ede7",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.plot(x1,x2,'ko')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40019b16",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "xall = np.array([x1,x2]).T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "795e773d",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "xall.shape"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4309915",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "xall[10]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85c84386",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# initialize the PCA object to only retain the first two principal components\npca = PCA(n_components=2)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f41ab02c",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# obtain the dimensionally reduced version of our dataset\nxall_reduced = pca.fit_transform(xall)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60753f9a",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.plot(xall_reduced[:,0],xall_reduced[:,1],'ko')\nplt.xlim(-100,100)\nplt.ylim(-100,100)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c6282b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "pca.explained_variance_ratio_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d903b7fb",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "pca.components_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d13346f8",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "pca.components_[0]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd67df3",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "fig,ax = plt.subplots(figsize=(5,5))\nplt.plot([0,100*pca.components_[0][0]], [0,100*pca.components_[0][1]], 'r')\nplt.plot([0,100*pca.components_[1][0]], [0,100*pca.components_[1][1]], 'b')\nplt.xlim(-110,110)\nplt.ylim(-110,110)\nplt.scatter(xall[:,0],xall[:,1],s=2)"
    },
    {
      "cell_type": "markdown",
      "id": "a39826e3",
      "metadata": {},
      "source": "As an applied example, I will use Seaborn (`sns`) to import the toy Penguins dataset and I will drop all rows that have NaNs:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8226f181",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "p = sns.load_dataset('penguins')\np = p.dropna()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f0f659",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# View the first 5 rows\np.head()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5d9ac3",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# Each species has measurements that fall into different regions of the feature space\nsns.pairplot(data=p,\n             hue='species')"
    },
    {
      "cell_type": "markdown",
      "id": "46ec5a00",
      "metadata": {},
      "source": "For clustering, we'll assume we DO NOT know what the species are, and we will only retain the numerical data."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6cf0fdb",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "p_data = p.select_dtypes(include='number')"
    },
    {
      "cell_type": "markdown",
      "id": "32c21425",
      "metadata": {},
      "source": "`p_data` has 333 data samples and 4 columns for the numerical features, as can be seen with the following:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9eff0de",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "p_data.shape"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "872d7201",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "p_data.head()"
    },
    {
      "cell_type": "markdown",
      "id": "0aaf4cf3",
      "metadata": {},
      "source": "This is a small dataset, but let's assume that we are EXTREMELY low on RAM and want to retain a lower number of features before doing our ML.\n\nWe can use PCA (Principal Components Analysis) to identify vectors in our feature space that retain a maximal amount of variance in our data and allow us to transform our coordinates in order to drop down into a lower-dimensional space.\n\n* Here's the documentation for Scikit-Learn's PCA\n  * https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n* As an input parameter, we can specify `n_components`\n  * If this is assigned an integer value, it will be the number of principal components to retain\n  * If this is assigned a float value between 0 and 1, it will be the percentage of variance to retain"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0b1ad2",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# initialize the PCA object to only retain the first two principal components\npca = PCA(n_components=2)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ead765",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# obtain the dimensionally reduced version of our dataset\np_data_reduced = pca.fit_transform(p_data)"
    },
    {
      "cell_type": "markdown",
      "id": "cf1ba13c",
      "metadata": {},
      "source": "* `p_data_reduced` is now a numpy array rather than a Pandas dataframe\n* The above code retains only the first two principal components, so now we'll have reduced the dimensionality of our feature space from 4 to 2:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1c889f",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "p_data_reduced.shape"
    },
    {
      "cell_type": "markdown",
      "id": "fc379cb0",
      "metadata": {},
      "source": "More precisely, the values in the two columns are now the values corresponding to the transformed coordinates along our first two principal components, and the other two principal components have been dropped.  The amount of variance in our data that is explained by each of the components is:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959a1971",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "pca.explained_variance_ratio_"
    },
    {
      "cell_type": "markdown",
      "id": "fd0ba838",
      "metadata": {},
      "source": "Which means that ~99.99% of the variance lies along the direction specified by the first PC, and 0.0078% by the second PC.  \n\nThis may seem to be too fortuitous that all the variance lies along one PC, and indeed it is -> one feature variable has a much larger range than the others and its variance overwhelms the other variables."
    },
    {
      "cell_type": "markdown",
      "id": "28ffefbb",
      "metadata": {},
      "source": "We could also operate by identifying the amount of variance we want to retain.  If we want to retain enough principal components to explain some percentage of the variance, we could write:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed11bba6",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "pca = PCA(n_components=0.95)\np_data_reduced = pca.fit_transform(p_data)"
    },
    {
      "cell_type": "markdown",
      "id": "0ae23e1e",
      "metadata": {},
      "source": "And we would now have a number of components to retain 95% of the variance.  The number can be found with:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21e0edff",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "pca.n_components_"
    },
    {
      "cell_type": "markdown",
      "id": "a84fc0cd",
      "metadata": {},
      "source": "And the dataset would be reduced in dimensionality to:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b376e7",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "p_data_reduced.shape"
    },
    {
      "cell_type": "markdown",
      "id": "215e65e9",
      "metadata": {},
      "source": "If all the variance is explained with just one principal component, this is usually a red flag -> it can mean that one variable has very large values compared to the others and its variance is much larger in magnitude, or it can mean that some variables are highly correlated.\n\nI will do one more processing step to put the variable values on a common scale with standard scaling:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb1be5d5",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "s = StandardScaler()\np_data_scaled = s.fit_transform(p_data)\n\n# and use the scaled data when reducing the dimensionality\npca = PCA(n_components=0.95)\np_data_reduced = pca.fit_transform(p_data_scaled)\n\nprint('Number of components: ', pca.n_components_)\nprint('Shape of reduced p_data_scaled: ', p_data_reduced.shape)\nprint('Variance explained by each PC: ', pca.explained_variance_ratio_)"
    },
    {
      "cell_type": "markdown",
      "id": "a7c5438e",
      "metadata": {},
      "source": "Now let's do the clustering.  To do K-Means Clustering, I will:\n* use Scikit-Learn's `KMeans` (which is already imported)\n  * https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n* specify the number of clusters to be 3\n* specify `n_init=10`, which means 10 trainings will be performed with different initial centroid seeds and the result will be that training which gives the best result in terms of inertia"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71f57a53",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "kmeans = KMeans(n_clusters=3, n_init=10)\n\n# Here the fit is done on the dimensionally-reduced data, not the whole dataset\nkmeans.fit(p_data_reduced)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13010622",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# add a new column to our original dataframe with the predicted cluster values\np['cluster'] = kmeans.predict(p_data_reduced)\n\n# plot the pairplot again with color representing the predicted cluster value (not the species value!)\nsns.pairplot(data=p,\n             hue='cluster')"
    },
    {
      "cell_type": "markdown",
      "id": "f23a67df",
      "metadata": {},
      "source": "The above colors represent the clusters identified by K-Means Clustering, and we have done the clustering using only 3 principal components rather than the 4 original features.  You can compare against the previous plot showing colors for species to see how well they line up."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}