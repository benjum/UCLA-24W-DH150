{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "702a52d0-2806-4a93-8243-2e488aec74be",
      "metadata": {},
      "source": "## Example of ML on Images:  Classifying Handwritten Digits"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14c49eba-4984-4d7c-852c-9f3315b8d649",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "import matplotlib.pyplot as plt\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics"
    },
    {
      "cell_type": "markdown",
      "id": "811fbf4c-ea6b-4acf-82f1-0e6466a1e1c6",
      "metadata": {},
      "source": "We use the toy digit dataset provided by scikit-learn.  \n\n(We will also find it fun later to try our hand at the full MNIST dataset, one of the classic initial problems for budding machine-learning practicioners.)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4483ce3b-9f1a-4c95-8d8c-584a4e0bbbb1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "d = sklearn.datasets.load_digits()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7614fcd-1d24-4ccb-96bb-f7fbe43b8450",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(d.DESCR)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3c23bbb-17b4-487c-abb7-8057737521fd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "x = d.data\ny = d.target"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dc9ad95-7a4f-4144-8442-9ea1285cd2ca",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "x.shape"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45562dc7-b7b2-496a-b19b-d2dc9bbb666e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "y.shape"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c87995c4-adc7-4ed1-bf4c-44c0c058c176",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "x[0]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c552e99-3f17-47d0-b8f0-19d77c4dfc9f",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "y[0]"
    },
    {
      "cell_type": "markdown",
      "id": "530aa893-ce0d-485d-bf3e-b1d3cffe114b",
      "metadata": {},
      "source": "The samples consist of 64 features, one for each pixel value of an 8x8 image array.  We can reshape the sample into an 8x8 array in order to visualize it."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d33b12-bd83-4c33-8245-b318894c4696",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "sample = x[4].reshape(8,8)\nplt.imshow(sample, cmap='binary')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d39beb-140b-4d74-9a06-d4d8867b6614",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "for i in range(100):\n    plt.subplot(10,10,i+1)\n    sample = x[i].reshape(8,8)\n    plt.imshow(sample, cmap='binary')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84bc854-8edf-4d36-8ff2-43197d35b0cd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n        x, y, test_size=0.2, random_state=42)"
    },
    {
      "cell_type": "markdown",
      "id": "59257867-907f-4f56-8f24-2d2b3c7a7775",
      "metadata": {},
      "source": "One catch to watch out for in splitting up your data into training and test sets:  stratification.\n\nLet's say you have a dataset that has 90% cat images and 10% dog images.  If you split your data and end up with 99% cats in your training data and 1% dogs, you'll be training your model on an unrepresentative sample.  (Sampling issues like this can be much more consequential and damaging than distinguishing cats from dogs!)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdbb5b75-2764-4572-b5f8-82765c30d9af",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.hist([y, y_train]);"
    },
    {
      "cell_type": "markdown",
      "id": "82a1e8c0-4a91-4118-a165-1250c3e07d59",
      "metadata": {},
      "source": "Here the difference in percentages is noticeable but not too significant by eye.  Nevertheless, we can stratify our split properly by including the \"stratify\" parameter and assigning it our target variable."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b7c4692-a0c8-4991-949e-7541a5bb34e0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(\n        x, y, test_size=0.2, random_state=42, stratify=y)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93234e5-0dd8-469e-9f31-86078c148430",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.hist([y, y_train]);"
    },
    {
      "cell_type": "markdown",
      "id": "a4a78858-15da-4bfe-bc09-a8a1b444109b",
      "metadata": {},
      "source": "## Logistic Regression"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb964029-aee4-4d0e-b9bb-cd7017cf59d5",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "import sklearn.linear_model\nlr_classifier = sklearn.linear_model.LogisticRegression()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc8bca1-5d79-4b59-b65a-5a716b282fbe",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "lr_classifier.fit(x_train, y_train)"
    },
    {
      "cell_type": "markdown",
      "id": "0e8d52bf-9812-4bcf-9091-03b0a9fb67d9",
      "metadata": {},
      "source": "It will not be uncommon for you to run into scenarios in which you encounter warnings or errors when trying to train models.\n\nIn such cases, they can be fruitful opportunities to consult the documentation and learn more about various training options.\n\nHere, the error message gives us clues about potentially insightful documentation.\n\nTo fast-forward, it will be useful here for Logistic Regression if we rescale our sample data from being integer values over [0:16] to being continuous values scaled to have a normal distribution of values -> the sklearn StandardScaler will rescale the features to have 0 mean and unit variance."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e18309-0f4e-4db0-883b-8d620926a76d",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "import sklearn.preprocessing"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50fcba6-7350-4d37-aa8b-77bd2b36d2b2",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "scaler = sklearn.preprocessing.StandardScaler()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4304d87-0b7d-44de-8afe-2b69fed14362",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "x_scaled = scaler.fit_transform(x_train)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1807c2fd-a0a8-44a4-9e56-8b97acf7e31d",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "x_scaled[0]"
    },
    {
      "cell_type": "markdown",
      "id": "f15deb6a-26b1-4556-8d50-ee52acdd813e",
      "metadata": {},
      "source": "Here's the difference in image between original and rescaled."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c165e98-ddcd-400e-afc0-bf22af694041",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "sample1 = x_train[7].reshape(8,8)\nsample2 = x_scaled[7].reshape(8,8)\n\nfig,ax = plt.subplots(1,2)\nax[0].imshow(sample1, cmap='binary')\nax[1].imshow(sample2, cmap='binary')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd33ad7-9e57-4dd5-b60c-a584a3c83255",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "for i in range(100):\n    plt.subplot(10,10,i+1)\n    sample = x_scaled[i].reshape(8,8)\n    plt.imshow(sample, cmap='binary')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68349695-2b06-47ba-8cb8-ca798cb288d0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "lr_classifier.fit(x_scaled, y_train)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "502a4cb5-0e73-44a7-bd19-528d33af1865",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "lr_classifier.predict(x_scaled[[7]])"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ee8e4d-d7de-48af-bf95-6b038921c3d4",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "y_train[7]"
    },
    {
      "cell_type": "markdown",
      "id": "1a283fc6-521b-424a-95a8-3ae1587596c9",
      "metadata": {},
      "source": "Our classifier was trained on scaled data, so we must scale any new data similarly (though we only need to do the transform now, not the fit.)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f963882-735f-4667-b25a-95dc292c0bed",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "x_test_scaled = scaler.transform(x_test)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bb8fca-4de4-4b3d-898b-b06e53cbe97b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "y_pred = lr_classifier.predict(x_test_scaled)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e25ef5",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(f\"Accuracy: {sklearn.metrics.accuracy_score(y_test, y_pred):.2%}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f6f814-d655-4855-8e8c-7f7039ed4158",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "cm = sklearn.metrics.confusion_matrix(y_test, y_pred)\ncm"
    },
    {
      "cell_type": "markdown",
      "id": "2792bad9",
      "metadata": {},
      "source": "In contrast with binary classification, calculating precision and recall (and etc) for multi-class classification problems can be computed in slightly different ways depending on how one does averaging. \n\nMicro-average:  equal importance to each instance.  Gives a global perspective of performance where overall performance is more important than class-specific performance.  \n\nMacro-average:  equal importance to each class.  Computes the metric independently for each class and then takes the average (hence treating all classes equally). Can be useful if you don't want the performance metric to be dominated by the performance of the majority class.\n\nWeighted average:  assign weights to each class before averaging.  Helpful if the performance on certain classes is critical or more reflective of real-world scenarios."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feae11f5",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "from sklearn.metrics import classification_report"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76a8164",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(classification_report(y_test, y_pred))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "392b31af",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(f\"Accuracy: {sklearn.metrics.accuracy_score(y_test, y_pred):.2%}\")\nprint(f\"Precision: {sklearn.metrics.precision_score(y_test, y_pred, average='micro'):.2%}\")\nprint(f\"Recall: {sklearn.metrics.recall_score(y_test, y_pred, average='micro'):.2%}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090a86f4",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(f\"Accuracy: {sklearn.metrics.accuracy_score(y_test, y_pred):.2%}\")\nprint(f\"Precision: {sklearn.metrics.precision_score(y_test, y_pred, average='macro'):.2%}\")\nprint(f\"Recall: {sklearn.metrics.recall_score(y_test, y_pred, average='macro'):.2%}\")"
    },
    {
      "cell_type": "markdown",
      "id": "0407c955",
      "metadata": {},
      "source": "# Is the learned model interpretable?"
    },
    {
      "cell_type": "markdown",
      "id": "24f15e1b",
      "metadata": {},
      "source": "Well.... we have a model and we've learned its coefficients:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d03fb0ee",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "lr_classifier.coef_.shape"
    },
    {
      "cell_type": "markdown",
      "id": "656070c5",
      "metadata": {},
      "source": "What are the 640 coefficients telling us?"
    },
    {
      "cell_type": "markdown",
      "id": "75f30544",
      "metadata": {},
      "source": "We've learned before that logistic regression was finding the coefficients for the logistic curve:\n$$ f(x) = \\frac{1}{1 + e^{-\\theta^T x}} $$\n\nThere are a few ways to tackle multi-category classification.  The default for logistic regression is to learn coefficients for the softmax function:\n$$ P(y = j | x) = \\frac{e^{\\theta_k^T x}}{\\Sigma_{k=1}^{K}{e^{\\theta_k^T x}}} $$\n\nThe basic gist for this problem is that there are:\n* 64 features (the pixels)\n* 10 classes (K = 10 for the softmax equation)\n* softmax is very nice because all probabilities sum to 1 and the class with the largest probability is the predicted class"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63f2b83",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "denominator = 0\nfor i in range(10):\n    denominator += np.exp((lr_classifier.intercept_[i] +\n                           np.dot(x_test[0], lr_classifier.coef_[i])))\nfor i in range(10):\n    prob = np.exp((lr_classifier.intercept_[i] +\n                   np.dot(x_test[0], lr_classifier.coef_[i]))) / denominator\n    print(\"P(y={:d}|x_test[0]) = {:.2e} = {:.2f}\".format(i,prob,prob))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ef7b5c",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "lr_classifier.predict_proba([x_test[0]])"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2372bea",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "np.sum(lr_classifier.predict_proba([x_test[0]]))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb3de539",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "np.argmax(lr_classifier.predict_proba([x_test[0]]))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f14af1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "lr_classifier.predict([x_test[0]])"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0966e2d9",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.bar(range(10),lr_classifier.predict_proba([x_test[0]])[0])"
    },
    {
      "cell_type": "markdown",
      "id": "916ef2a1",
      "metadata": {},
      "source": "It's good to know the equation, but that's a complex equation with a lot of coefficients to interpret.\n\nCan we get insights into why a number got misclassified?"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4191bd7e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "elem = 0\nplt.imshow(x_test[elem].reshape(8,8), cmap='binary');\nprint('Real value:',y_test[elem])\nprint('Predicted value:',lr_classifier.predict([x_test[elem]]))"
    },
    {
      "cell_type": "markdown",
      "id": "d27306b5",
      "metadata": {},
      "source": "It may be useful to consider some ways to interpret logistic regression for a simpler problem."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}