{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f5f6b6c-e8c7-4fa1-95f7-880e66c46830",
   "metadata": {},
   "source": [
    "# Cats vs Dogs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61602b9-dadc-4e3d-ab17-c45a1ba2b643",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/benjum/UCLA-24W-DH150/main/Data/cat-dog-data/test/cat/cat.1500.jpg\" width=\"250px\"/> <img src=\"https://raw.githubusercontent.com/benjum/UCLA-24W-DH150/main/Data/cat-dog-data/test/dog/dog.1500.jpg\" width=\"250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9deb23d-7f8b-46a3-9032-3aa37350d462",
   "metadata": {},
   "source": [
    "#### We're going to use TensorFlow and Convolutional Neural Networks to identify whether a picture is a dog or a cat.\n",
    "\n",
    "Side-note:  I think the dataset is biased against cats, because most of the cats I've looked at in this dataset look crazy, a bit ugly, or like they totally have it out for the dogs!\n",
    "\n",
    "This dataset is a subset of the data that can be obtained at [Kaggle's Dogs Vs Cats page](https://www.kaggle.com/c/dogs-vs-cats/data).\n",
    "\n",
    "The code below was motivated by Chollet's Deep Learning with Python book, in which you will find many, many more interesting details and tidbits about doing deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25feba02-7adf-40f4-b7e8-08c1c79d495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac9a55-3e84-46eb-b79a-334e5b5b0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061dd3d-0791-4e8d-8a50-9316d82c0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7bafc2-5805-4593-838b-639021289f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa032d6-3d69-41b8-bb6f-845ffa577551",
   "metadata": {},
   "source": [
    "## Getting our data\n",
    "\n",
    "Our data images are in folders that are already split up into training, test, and validation sets.  Furthermore, they are aleady split up into cat and dog folders.\n",
    "\n",
    "The following image_dataset_from_directory gets class labels based on whether the images are retrieved from these \"cat\" or \"dog\" folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e98d3e-75aa-45dd-8d88-850309479a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec79054-3acb-4687-b3b8-39f97f6b21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(os.getcwd(), \"..\", \"..\", \"Data\", \"cat-dog-data\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552767ec-47ff-4832-89cc-9342a92ec330",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    os.path.join(os.getcwd(), \"..\", \"..\", \"Data\", \"cat-dog-data\", \"train\"),\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    os.path.join(os.getcwd(), \"..\", \"..\", \"Data\", \"cat-dog-data\", \"validation\"),\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    os.path.join(os.getcwd(), \"..\", \"..\", \"Data\", \"cat-dog-data\", \"test\"),\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7b04a-1887-45ff-9d6a-dd921fade669",
   "metadata": {},
   "source": [
    "Brief aside:  when you have these batches of data, how can you work with these Python data objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78c38b-30c3-49fb-a293-811f58a219a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_dataset.unbatch()\n",
    "a = list(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be29367c-462a-44ac-9868-765526be929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfdf761-6611-4c2e-bd30-a60eb89e371d",
   "metadata": {},
   "source": [
    "The image is at index 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463f324-1cfb-4b7f-a7dd-3fa80463c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248abb33-6882-48d5-a42f-4fcf961d93b8",
   "metadata": {},
   "source": [
    "The label is at index 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a4091-2c76-4b34-afc3-39f71ef6f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420df15-f3ea-436d-900a-0d79b896c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2][0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d709a-8398-4427-aabf-2b7005195648",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc161d-7258-428c-987b-a14e3272c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1002\n",
    "print(\"The label for image\",b,\"is\",a[b][1].numpy())\n",
    "print(\"The picture for image\",b,\"is\")\n",
    "plt.imshow(a[b][0].numpy().astype('int32'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e875d5a-f2a9-4914-a69e-1bb13b3e3e8b",
   "metadata": {},
   "source": [
    "## Building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f1bcb-56ed-4353-9c97-1c9be6bcf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddce681-f782-4b12-939e-0211c0a9ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a873cb-c76c-4f9c-a6a1-b3da1253141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b61445-3fa1-48ce-b258-d32642415e01",
   "metadata": {},
   "source": [
    "Note that since we are doing binary classification, we now use binary_crossentropy rather than categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0654ad-e032-4a99-b834-abbddf3f056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d70b4a-6800-4fbf-abaa-21470579177f",
   "metadata": {},
   "source": [
    "During our model fitting, we can save the model in a file after each epoch.  This allows us to retrieve model information later if needed.\n",
    "* Key to our needs, when we use `save_best_only=True` and `monitor=\"val_loss\"`, we only save the model into the file (and overwrite the previous model) if the current value of the val_loss metric is lower. \n",
    "* Our saved file thus saves that model corresponding its best performance on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb913540-54b2-42ae-ba36-a2bca84d1b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        # for below to work with gpu and tf 2.13\n",
    "        # save_weights_only=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ecdca-dcba-4378-bea5-2ab3f5e5d26d",
   "metadata": {},
   "source": [
    "Do the fit!\n",
    "\n",
    "Note that we include validation now as a part of this fit.  Validation is useful for assessing whether the trained model generalizes well to unseen data, and we do it on data that is not the test data because we may want to alter our model hyperparameters before doing our final training and then our final testing on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4eb93b-5121-4087-9432-f27efec33d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca24d3-995c-4c19-bdf1-ff8800623191",
   "metadata": {},
   "source": [
    "## How does the model perform?\n",
    "\n",
    "* Does it overfit to the training data?\n",
    "    * Does the accurarcy and loss on the validation data match up reasonably with the accuracy and loss on the training data?\n",
    "* What is the accuracy of classification?\n",
    "    * Assess the model on the test data (potentially only using the number of epochs for which validation has shown us that overfitting is not occurring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d784b-277e-49b1-8f13-6e5c77d9f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c15a45-6622-402f-a351-cd91b419816b",
   "metadata": {},
   "source": [
    "To test the model accurracy on test data, we take advantage of the fact that we've saved the model (in \"convnet_from_scratch.keras\") at that point for which the \"val_loss\" was lowest.\n",
    "* We can assess the model from a point before we started overfitting\n",
    "* We can do it without having to retrain the model completely using a smaller number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e0c5e-6735-47ce-a412-a15b116742af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset) \n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dcd371-068e-470c-bd21-458589d0ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with gpu and tf 2.13\n",
    "\n",
    "# model.load_weights(\"convnet_from_scratch.keras\")\n",
    "# test_loss, test_acc = model.evaluate(test_dataset) \n",
    "# print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd35ed4-43fa-4981-84eb-7fb56722add5",
   "metadata": {},
   "source": [
    "## Model #2\n",
    "\n",
    "The overfitting occurs in part because we have such a small dataset.  To bypass this without getting a lot of new data, we're going to use a clever bit of data manipulation.\n",
    "\n",
    "Rather than using new images, we're going to use the same images with a small bit of zoom, rotation, and/or horizontal flipping.  This allows us to get images that have different pixel values and that are new images to the algorithm, but that are essentially the same dog or cat image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856d634-9c3f-4fe8-b524-01f136bb68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e514839-0c51-43eb-94d6-24166ee1c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a2e79-8332-478d-8100-270e6e737270",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e647f6-d9ee-4197-8209-49600a9c8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9361281-3c50-46be-a347-6fdee020d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748adf7-5b0b-4719-bf68-2cbd57fee9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        # with gpu and tf 2.13\n",
    "        # save_weights_only=True\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=\"./tf_logs\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa41d47-d631-496d-afba-bd6d85ffb729",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    #epochs=20,\n",
    "    epochs=60,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1eb14-dac5-47d5-b8d6-1be08f58ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f9d67-92fb-4f25-8201-b4a0692d5735",
   "metadata": {
    "tags": []
   },
   "source": [
    "Much better behavior relative to overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af48727-926b-4c62-972c-64c76c5e796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"convnet_from_scratch_with_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset) \n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ae6e1-e3a3-4495-8731-c68e0c6c103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with gpu and tf 2.13\n",
    "\n",
    "# model.load_weights(\"convnet_from_scratch_with_augmentation.keras\")\n",
    "# test_loss, test_acc = model.evaluate(test_dataset) \n",
    "# print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69a7c4-58ac-4a08-b5d0-48d9d9524b7a",
   "metadata": {},
   "source": [
    "Getting better at accuracy.\n",
    "\n",
    "We can actually do much better with more convolutional and pooling layers! (although the computational demand for resources will be even higher -- these networks can quickly consume resources!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786a615-23f8-4f54-9571-e1512384b693",
   "metadata": {},
   "source": [
    "## Checking out a couple images and their cat/dog classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c251469-5874-43a1-85cb-4ae9aed01c5e",
   "metadata": {},
   "source": [
    "The test (and train and validation) data is stored in special TensorFlow batch data structures (retrieved from the file hierarchy).... these aren't exactly easy to unwind, but the following couple cells will allow us to retrieve example images and feed them into the model for making a classification prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61a68c-c486-40ff-b327-8c8c91a65f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.unbatch()\n",
    "a = list(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39f2ec-41a4-4906-a031-3601f9779e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b480b43-259b-4a12-847b-75f1128345c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 5\n",
    "print(\"The label for image\",b,\"is\",a[b][1].numpy())\n",
    "print(\"The picture for image\",b,\"is\")\n",
    "plt.imshow(a[b][0].numpy().astype('int32'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667792d4-aa47-4eba-b2f1-571d81607acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.predict(a[5][0].numpy().reshape(-1,180,180,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01cadf-4761-49e0-8f6a-301a241fe98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1051\n",
    "print(\"The label for image\",b,\"is\",a[b][1].numpy())\n",
    "plt.imshow(a[b][0].numpy().astype('int32'))\n",
    "if int(test_model.predict(a[b][0].numpy().reshape(-1,180,180,3))[0][0]>0.5) == 0:\n",
    "    print('Predict Cat')\n",
    "else:\n",
    "    print('Predict Dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8263c37e-6523-4165-8deb-bb79c4f79125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with gpu and tf 2.13\n",
    "\n",
    "# b = 1051\n",
    "# print(\"The label for image\",b,\"is\",a[b][1].numpy())\n",
    "# plt.imshow(a[b][0].numpy().astype('int32'))\n",
    "# if int(model.predict(a[b][0].numpy().reshape(-1,180,180,3))[0][0]>0.5) == 0:\n",
    "#     print('Predict Cat')\n",
    "# else:\n",
    "#     print('Predict Dog')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
